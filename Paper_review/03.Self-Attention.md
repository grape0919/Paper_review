# Self-Attention
 > - [Attention is All You Need](https://www.youtube.com/watch?v=mxGCEWOxfe8)
 
 <!--![Attension architecture](/img/8.png){ width=50% }-->
 <!--<img src="/img/8.png" alt="Attension architecture" style="width:50%;"/>-->
 
## self-attention model은 어떻게 나왔나?
 
 ![self-attention](/img/10.png)
 
 - 문맥에 따라 동적으로 할당되는 encode의 attention weight로 dynamic context vector 를 휙득
 - 기존 Seq2Seq의 encoder, decoder 성능을 비약적으로 향상 시킴
 - __하지만, 여전히 rnn이 순차적으로 연산이 이루어져 속도가 느림__
 
 ![self-attention model](/img/11.png)

### -> RNN 을 제거하면?

 - Attention is All You need
 - RNN 을 encoder와 decoder에서 제거
 
 ![self-attention model2](/img/12.png)

 - decoder가 해석하기에 적합한 weight를 찾음
  


## Self-Attention Model

 - 스스로를 잘 표현할 수 있는 어텐션을 학습하는 것을 목적으로 한다.
 - 조금 복잡한 테크닉을 거친다.
 - 1 단계 : 안어 임베딩
 - 2 단계 : concat
 - 3 단계 : query,key,value weight 계산
 
 ![self-attention model3](/img/13.png)

 - 4개의 토큰을 3종류의 vector로 표현
 - Query * Key^t 는 Attention score
 - Attention score에 Softmax를 취해주면, Attention Weight
 ![self-attention model4](/img/14.png)
 - 각 토큰마다 Context Vector 를 구할 수 있다.
 ![self-attention model5](/img/15.png)

 ![self-attention model6](/img/16.png)
 
## Multi Head Attention

 - Query, Key, Value로 구성된 attention layer를 동시에 여러개를 수행
 
 ![self-attention model7](/img/17.png)

## Multi Head Attention Encoder layer

 - 입력 vector 크기와 출력 vector 크기가 같다.

 ![self-attention model7](/img/18.png)
 
 
 
 
 
