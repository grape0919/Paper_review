# Attention 모델
 - 인간이 정보처리를 할 때, 모든 sequence를 고려하면서 정보처리를 하는 것이 아님
 - 인간의 정보처리와 마찬가지로, 중요한 feature는 더욱 중요하게 고려하는 것이 Attention의 모티브

 ![Attention 예시](/OCR/img/6.png)

 - 기존 Seq2Seq 에서는 RNN의 최종 output인 Context vector만 활용
 - Attention 에서는 인코더 RNN 셀의 각각 output활용
 - Decoder 에서는 매 step 마다 RNN 셀의 ouput을 이용해 dynamic 하게 Context vector 를 생성
 > - reference : NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE
 > - [[딥러닝 기계번역] 시퀀스 투 시퀀스 + 어텐션 모델](https://www.youtube.com/watch?v=WsQLdu2JMgI)

 ![Attention 예시2](/OCR/img/6-2.png)

 - 기존 rnn 모델에서 시작
 - rnn셀의 각 output들을 입력으로 하는 Feed forward fully connected layer
 - 해당 layer의 output을 각 rnn 셀의 Score로 결정
 
 > - _feed forword network_ 란?> 순방향 추론 또는 인코딩 과정
 - _fully connected layer_ 란?> 레이어간 모든 노드가 연결되는 레이어
 
 ![Attension model1](/OCR/img/7.png)

 - 출력된 score에 softmax를 취함으로써 0-1 사이의 값으로 변환
 - 해당 값을 Attention weight로 결정
 - Attension weight 와 hidden state 를 곱해서 Context vector 휙득
 
 ![Attension model2](/OCR/img/7-1.png)

 ![Attension model3](/OCR/img/9.png)
 
 - Decoder 의 hidden state 가 attention weight 계산에 영향을 줌
 
 ![Attension model4](/OCR/img/9-1.png)

#### Reference
 > - [Attention is All You Need](https://www.youtube.com/watch?v=mxGCEWOxfe8)

