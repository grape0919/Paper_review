# Bi-directional Encoder Representations from Transformers
- BERT는 bi-directional Transformer로 이루어진 언어모델
- 잘 만들어진 BERT 언어모델 위에 1개의 classification layer만 부착하여 다양한 NLP task를 수행
- 영어권에서 11개의 NLP task에 대해 state-of-the-art (SOTA) 달성

![bert](/img/25.png)


## Model architecture
- Contextual representation of token
- Transformer layer
- Input embedding layer
![bert](/img/26.png)


## 학습데이터
- 학습 코퍼스 데이터
  - BooksCorpus (800M words)
  - English Wikipedia (2,500M words without lists, tables and headers) • 30,000 token vocabulary

- 데이터의 tokenizing 
  - WordPiece tokenizing
  `He likes playing→He likes play ##ing`
  - 입력 문장을 tokenizing하고, 그 token들로 ‘token sequence’를 만들어 학습에 사용
  - 2개의 token sequence가 학습에 사용
![bert](/img/27.png)

## word piece tokenizer
- Byte Pair Encoding (BPE) 알고리즘 이용
- 빈도수에 기반해 단어를 의미 있는 패턴(Subword)으로 잘라서 tokenizing
![bert](/img/28.png)

## BPE 순서도
1. 모든 캐릭터 단위로 분리
2. bi-gram으로 합쳐 빈도수가 많이 등장하는 pair를 찾는다.
3. 찾은 pair를 vocab에 등록

![bert](/img/29.png)
 
## Masking 기법
- 데이터의 tokenizing
  - Masked language model (MLM): input token을 일정 확률로 masking
- input 형태
  - 학습데이터의 문장 맨 앞에 [CLS] 마지막에 [SEP]를 붙인다.
  - input token중 15%를 랜덤하게 선택한다.
  - 80%는 token을 masking 
  - 10%는 다른 token으로 대체
  - 10%는 그대로 사용

![bert](/img/30.png)

- 출력에는 인풋을 예측하도록 학습이 이루어짐.
- 한번에 2가지가 학습된다.
  - transformer layer 를 통해 다음 sentence를 예측
  - mask된 input을 통해 다음 단어를 예측
![bert](/img/31.png)

- input 으로는 token 자체가 들어가는 것은 아니고,
- token에 대해서 embedding layer를 하나 거친다.
  1. token word embedding
  2. segment embedding 
    - 문장 A인지 문장 B인지
  3. position embedding 
    - input 만들어가면 transformer에서 token에 대한 순서를 알 수 없다.
 
![bert](/img/32.png)

## NLP 실험

![bert](/img/33.png)

## Results
![bert](/img/34.png)
![bert](/img/35.png)
![bert](/img/36.png)

## Keystone

### Unsupervised LM
 - Masked LM : 입력의 특정 %의 단어를 랜덤하게 마스킹하여 추론하도록 학습
 - Next Sentence Prediction : QA나 자연어 추론을 위해서 문장간의 관계를 이해시키도록 함. 다음 문장이 이어지는 문장인지 아닌지를 학습

### Transfer learning 의 도입
 - pre training 된 LM 을 필요한 Task에 맞게 fine-tuning 하여 사용함.
 - 효휼성 향상, 정확도 향상



