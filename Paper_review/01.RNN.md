# 초기 언어 모델
## Markov 확률 기반의 모델(Markov Chain Model)
 - 다음의 단어나 문장이 나올 확률을 통계와 단어의 n-gram을 기반으로 계산
 - 딥러님 기반의 언어모델은 해당 확률을 최대로 하도록 네트워크를 학습

 ![전이 확률 그래프](/OCR/img/2.png)

---

# rnn (Recurrent Neural Network)
 - RNN은 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는 인공신경망의 한 종류
 - 이전 state 정보가 다음 state를 예측하는데 사용됨으로써 시계열 데이터 처리에 특화됨
 
 ![rnn의 구조](/OCR/img/3.png)
 ![rnn의 구조2](/OCR/img/3-1.png)

 - RNN 기반의 Seq2Seq(Encorder - Decorder)
 - 한국어로 인코딩하고 영어로 디코딩하면 번역기
 
 ![encoder-decoder](/OCR/img/4.png)

### RNN의 구조적 문제점
 - 입력 sequence의 길이가 매우 긴 경우, 처음에 나온 token에 대한 정보가 희석
 - 고정된 context vector 사이즈로 인해 긴 sequence에 대한 정보를 함축하기 어려움
 - 모든 token이 영향을 미치니, 중요한지 않은 token도 영향을 줌
 
 ![rnn 문제점](/OCR/img/5.png)
 
 - 문서를 볼때 주요키워드만 본다.
 - 중요하지 않은 키워드도 영향을 미친다.
 - 이것이 rnn 네트워크의 한계점
 - 그래서 나온것이 attention 모델

#### Reference
> https://www.youtube.com/watch?v=qlxrXX5uBoU
