## multi head attention encorder layer
- encoder layer 는 입력 vector와 출력 vector 크기가 같다

![transformer encoder](/img/18.png)

- Transfomer 는 6개의 Multi-Head-Attention encoder layer로 구성

![transformer encoder](/img/19.png)

## decoder layer

- decorder 도 encoder와 똑같이 6개의 레이어로 구성됨

![transformer encoder](/img/20.png)
![transformer encoder](/img/21.png)

## encoder layer 와 decoder layer 비교

![transformer encoder](/img/22.png)

- encoder 와 다르게 decoder 의 첫번쨰레이어는 Masked Multi head Attention 레이어이다.
- Masked Multi head Attention은 지금까지 출력된 값에 대해서만 적용하기 위해서.
- decoder 에서는 Multi head Attention layer에서는 decoder의 현재상태를 query로
  encoder의 key,value를 사용해서 decoder의 다음  단어를 예측

![transformer encoder](/img/23.png)

#### Reference 
 > - [Attention is All You Need](https://www.youtube.com/watch?v=mxGCEWOxfe8)
