***
### 읽기전 알아야하는 기술들

> - Word Embedding
- word piece model
- rnn
- encoding - decoding
- seq2seq
- attention mechanism
- transformer
- multi-head self attention
- BERT
- Vanila LayoutLM

***

# LayoutLM
사전 훈련 기술은 최근 몇 년 동안 다양한 NLP 작업에서 성공적으로 검증되었습니다. NLP 애플리케이션에 대한 사전 학습 모델이 널리 사용됨에도 불구하고 문서 이미지 이해에 필수적인 레이아웃 및 스타일 정보는 무시하면서 거의 전적으로 텍스트 수준 조작에 집중합니다. 이 논문에서는 **스캔 한 문서 이미지에서 텍스트와 레이아웃 정보 간의 상호 작용을 공동으로 모델링하는 LayoutLM을 제안**합니다. 이는 **스캔 한 문서에서 정보를 추출하는 것과 같은 많은 실제 문서 이미지 이해 작업에 유용**합니다. 또한 이미지 기능을 활용하여 단어의 시각적 정보를 LayoutLM에 통합합니다. 우리가 아는 한, 문서 수준의 사전 교육을 위해 단일 프레임 워크에서 텍스트와 레이아웃을 공동으로 학습 한 것은 이번이 처음입니다. 양식 이해 (70.72에서 79.27), 영수증 이해 (94.02에서 95.24) 및 문서 이미지 분류 (93.07에서 94.42)를 포함한 여러 다운 스트림 작업에서 새로운 최신 결과를 달성합니다.

## Archtecture
![layoutLMv2 architacture](/img/37.png)

## results

LayoutLM 모델을 사전 학습 된 두 개의 SOTA NLP 모델 인 BERT 및 RoBERTa [16]와 비교합니다. BERT BASE 모델은 0.603을 달성하고 LARGE 모델은 F1에서 0.656을 달성합니다. BERT와 비교할 때 RoBERTa는 더 많은 에포크가있는 더 큰 데이터를 사용하여 훈련되므로이 데이터 세트에서 훨씬 더 나은 성능을 발휘합니다. 시간 제한으로 인해 LayoutLM에 대해 4 가지 설정을 제공합니다. 이는 6 Epoch의 500K 문서 페이지, 6 Epoch의 1M, 6 Epoch의 2M, 2 Epoch의 11M입니다. LayoutLM 모델은 기존 SOTA 사전 교육 기준을 상당히 능가하는 것으로 나타났습니다. BASE 아키텍처를 사용하면 학습 데이터가 11M 인 LayoutLM 모델은 F1에서 0.7866을 달성하여 비슷한 크기의 매개 변수를 사용하는 BERT 및 RoBERTa보다 훨씬 높습니다. 또한 사전 훈련 단계에서 MDC 손실을 추가하고 FUNSD 데이터 세트에 상당한 개선을 가져옵니다. 마지막으로 LayoutLM 모델은 텍스트, 레이아웃 및 이미지 정보를 동시에 사용할 때 0.7927의 최고의 성능을 달성합니다. 


![layoutLMv2 architacture](/img/39.png)


# LayoutLMv2
# 101. LayoutLMv2 for VrDU(Visually-rich Document Understanding)

[2012.14740.pdf.zip](101%20LayoutLMv2%20for%20VrDU(Visually-rich%20Document%20Und%20782c9cf4ce4e48cb947d3d5e6caac6f5/2012.14740.pdf.zip)

오역, 의역 있을

# Abstract

더 잘 배워진 [cross-modality](https://www.notion.so/cross-modality-20b8ab6d07cc4fa4be694e01adcc73c7)  interaction 에서, 현재 사용되는 masked visual-language modelling 뿐만 아니라 pre-training 에서 새로운 text image alignment 와 matching Task를 사용한다.

한편으로는, 우선, 이것은 spatial-aware [Self-attention Mechanism](https://www.notion.so/Self-attention-Mechanism-3796849904a64ded9b08839550330136) 을 Transformer architecture 로 통합하는 것으로, 모델은 text-blocks 중에 관련있는 위치적 관계 또한 완벽하게 이해할 수 있다.

# 1. Introduction

VrDU 는 스캔되거나 디지털 형태의 비즈니스 문서(자동적으로 추출될 수 있거나 조직화 될 수 있는 구조화된 정보)를 분석하는 것이 목적이다. 많은 비즈니스 어플리케이션을 위해.

기존의 정보추출 작업과는 달리, VrDU작업은 textual 정보에 의지할 뿐만 아니라, 시작적으로 풍부한 문서를 위한 vital인 정보의 visual 과 layout도 의지한다.

다른 타입의 문서들은 문서 내에서 다른 위치에 놓인 종종 스타일이나 포맷을 결정하는 텍스트 필드를 가리킨다.

그러므로 정확게 관심있는 텍스트 필드 인식하는 것은 분명히 시각적으로 풍부한 문서들의 교차 양식 성질의 이점을 가져온다. 텍스쳐, 비쥬얼과 조인트하게 모델되거나 싱글 프레임워크에서 end-to-end로 학습될 수 있는 레이아웃 정보에서.

최근 VrDU의 프로세스가 주로 두 방향에서 드러나고 있다. 첫번때 방향은 대게 텍스처, 비쥬얼/레이아웃/스타일 정보사이가 얕은 융합에서 빌드되었다. 

이 것들은 프리 트레인된 NLP, CV 모델들에 개별적으로 접근하고 supervised leaning(지도학습)을 위해 multiple 양식의 정보을 조합한다.

성능은 좋았지만 변경된 한 문서로 인해 재학습이 필요하다.

추가적으로, 한 문서 타입에서의 도메인 지식이 쉽게 다른 문서타입으로 변환될 수 없다. 때문에 일반적인 문서 레이아웃에서의 지역 불변성은 완전하게 이용할 수 없다.

이 논문에서는...

텍스트 기반 프리트레인 모델과 달리 2D position embedding, image embedding, 기존 text embedding을 사용한다.

**pre-training 단계에서는 두 트레이닝이 사용된다.**

**1) masked visual-language model**

**2) multi-label document classification**

모델은 pre-trained 되었다/많은 수의 IIT-CDIP 데이터셋 unlabeled scanned document images 로.

, 그리고 몇몇 downstream task들에서 기대할 만한 결과를 얻을 수 있다.

기존 연구의 연장은, 우리는 제안한다/새로운 모델 아키텍처 와 pre-training 들을/ LayoutLMv2 모델에서.

- **vanilla LayoutLM 모델과 다른점**은 / 이미지 임배딩 에서 / fine-tuning stage에서 결합되는,
우리는 통합한다/  이미지 정보를/LayoutLMv2의 프리트레인 단계에서 /정보사이에서 cross-modality를 배운 Transformer architecture 의 이점가져오는 것/이미지와 텍스처 정보를

    Different from the vanilla LayoutLM model where image embeddings are combined in the fine-tuning stage, we integrate the image information in the pre-training stage in LayoutLMv2 by taking advantage of the Transformer architecture to learn the cross-modality interaction between visual and textual information.

추가적으로, 1차원 상대 위치 표현에서 영감을 받아, 우리는 제안한다/ LayoutLMv2를 위한 spatial-aware self-attention mechanism 을/ LayoutLMv2-토큰 쌍을 위한 2차원 상대 위치 표현이 포함된.

2-D 포지션 임베딩이랑 다른점은, 상대 위치 임배딩은 명시적으로 제공한다 / broader view를 / 문맥상 공간 모델링을 위한.

프리트레이닝에서 2가지 새로운 트레이닝을 한다.

첫째, 제안된다/ 텍스트이미지 정렬 계획-이미지에서 텍스트라인을 커버하고 텍스트 사이드에서 토큰이 이미지사이드에 커버 되는지 아닌지 분류 예측을 만드는- 이다.

둘째, 텍스트 이미지 매칭 계획이다.여기서 모델을 만들기 위헤 텍스트-이미지 쌍의 일부 이미지는 다른 문서 이미지로 임의로 대체되어 영상 및 OCR 텍스트가 상관 있는지 여부를 학습하게 합니다.

- Downstream Task 란

    구체적으로 풀고 싶은 문제들을 말한다.

    최근 자연어 처리분야에서 언어모델을 프리트레인 방식을 이용해 학습을 진행하고, 그 후에 **실제로 하고자하는 TASK**를 파인 튜닝 방식을 통해 모델을 업데이트하는 등의 방식을 사용하는데 이때 **TASK**를 Downstream Task라고 한다.

- state-of-the-art

    일명 SOTA, 분야별 딥러닝 모델 정보, 데이터셋 등이 제공되는 플랫폼

    [https://paperswithcode.com/sota](https://paperswithcode.com/sota)

실험 결과는 보여준다/ LayoutLMv2 모델의 뛰어난 강력한 baselines 성능을/ vanilla LayoutLM을 포함해서/ 그리고 새로운 SOTA 결과들을 얻었다/ 이런 downstream VrDU Task들에서, 충분히 이익이 되는 많은 수의 실제 문서 이해 tasks.

논문의 기여 요약:

1. single framework에서 end-to-end로 [cross-modality](https://www.notion.so/cross-modality-20b8ab6d07cc4fa4be694e01adcc73c7)  interaction을 학습하는 문서 text, layout, image 정보를  pre-training 단계에서 통합하기 위한 multi-modal Transformer 모델을 제안한다.
2. masked visual-language model외에도, 우리는 다른 양상(modality) 사이를 정렬을 강제하기 위한, 텍스트 이미지 매칭과 택스트 이미지 정렬하는 새로운 프리트래이닝 전략도 더한다. 한편, 공간 인식 self-attention mechanism도 Transformer architecture로 통합된다.
3. LayoutLMv2 는 기존의 VrDU task들의 강력한 성능의 baseline models 뿐 만 아니라, VrDU에 대해 multi-modal pre-training 의 큰 잠재력을 보여주는 새로운 SOTA 결과를 얻었다.

## 2. Approach

![layoutLMv2 architacture](/img/38.png)

제안된 LayoutLMv2 의 전반적인 형태는 보여준다/ [Figure2]() 에서. 우리는 이섹션에서 모델 아키텍처와 프리트레이닝 tasks를 소개할 것이다.

### 2.1 Model Architecture

multi-modal transformer 를 구축.

텍스트, 이미지, 레이아웃 세 입력을 허용한다.

각 입력들은 임베딩 시퀀스로 변환된다.

**텍스트 임베딩 : WordPiece 토크나이즈 사용한다. 단어를 토크나이즈 하고 맨앞에 [CLS] 토큰 추가, word가 끝날때는 [SEP] 토든을 넣고 맨 끝 [SEP] 토큰 뒤에는 [PAD]를 넣는다.**

$$S = \{[CLS],w1,w2,...,[SEP],[PAD],[PAD],...\}, |S| = L$$

### 2.2 Pre-Training

### 2.3 Fine-Tuning

#### Reference
> - LayoutLM: Pre-training of Text and Layout for Document Image Understanding
> - LAYOUTLMV2: MULTI-MODAL PRE-TRAINING FOR VISUALLY-RICH DOCUMENT UNDERSTANDING







