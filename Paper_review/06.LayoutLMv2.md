***
### 읽기전 알아야하는 기술들

> - Word Embedding
- word piece model
- rnn
- encoding - decoding
- seq2seq
- attention mechanism
- transformer
- multi-head self attention
- BERT
- Vanila LayoutLM

***

# LayoutLM
사전 훈련 기술은 최근 몇 년 동안 다양한 NLP 작업에서 성공적으로 검증되었습니다. NLP 애플리케이션에 대한 사전 학습 모델이 널리 사용됨에도 불구하고 문서 이미지 이해에 필수적인 레이아웃 및 스타일 정보는 무시하면서 거의 전적으로 텍스트 수준 조작에 집중합니다. 이 논문에서는 **스캔 한 문서 이미지에서 텍스트와 레이아웃 정보 간의 상호 작용을 공동으로 모델링하는 LayoutLM을 제안**합니다. 이는 **스캔 한 문서에서 정보를 추출하는 것과 같은 많은 실제 문서 이미지 이해 작업에 유용**합니다. 또한 이미지 기능을 활용하여 단어의 시각적 정보를 LayoutLM에 통합합니다. 우리가 아는 한, 문서 수준의 사전 교육을 위해 단일 프레임 워크에서 텍스트와 레이아웃을 공동으로 학습 한 것은 이번이 처음입니다. 양식 이해 (70.72에서 79.27), 영수증 이해 (94.02에서 95.24) 및 문서 이미지 분류 (93.07에서 94.42)를 포함한 여러 다운 스트림 작업에서 새로운 최신 결과를 달성합니다.

## Archtecture
![layoutLMv2 architacture](/img/37.png)

## results

LayoutLM 모델을 사전 학습 된 두 개의 SOTA NLP 모델 인 BERT 및 RoBERTa [16]와 비교합니다. BERT BASE 모델은 0.603을 달성하고 LARGE 모델은 F1에서 0.656을 달성합니다. BERT와 비교할 때 RoBERTa는 더 많은 에포크가있는 더 큰 데이터를 사용하여 훈련되므로이 데이터 세트에서 훨씬 더 나은 성능을 발휘합니다. 시간 제한으로 인해 LayoutLM에 대해 4 가지 설정을 제공합니다. 이는 6 Epoch의 500K 문서 페이지, 6 Epoch의 1M, 6 Epoch의 2M, 2 Epoch의 11M입니다. LayoutLM 모델은 기존 SOTA 사전 교육 기준을 상당히 능가하는 것으로 나타났습니다. BASE 아키텍처를 사용하면 학습 데이터가 11M 인 LayoutLM 모델은 F1에서 0.7866을 달성하여 비슷한 크기의 매개 변수를 사용하는 BERT 및 RoBERTa보다 훨씬 높습니다. 또한 사전 훈련 단계에서 MDC 손실을 추가하고 FUNSD 데이터 세트에 상당한 개선을 가져옵니다. 마지막으로 LayoutLM 모델은 텍스트, 레이아웃 및 이미지 정보를 동시에 사용할 때 0.7927의 최고의 성능을 달성합니다. 

![layoutLMv result](/img/39.png)

### 또한 표 2에 나와있는 FUNSD 데이터 세트에서 다른 데이터 및 에포크를 사용하여 LayoutLM 모델을 평가합니다.
다른 데이터 설정의 경우 사전에 더 많은 에포크가 훈련됨에 따라 전체 정확도가 단조롭게 증가 함을 알 수 있습니다. 훈련 단계. 또한 LayoutLM 모델에 더 많은 데이터가 공급됨에 따라 정확도도 향상됩니다. FUNSD 데이터 세트에는 미세 조정을위한 149 개의 이미지 만 포함되어 있으므로 결과는 특히 낮은 리소스 설정에서 스캔 된 문서 이해에 텍스트 및 레이아웃의 사전 학습이 효과적임을 확인합니다.

![layoutLMv result2](/img/40.png)

### 또한 처음부터, BERT 및 RoBERTa를 포함하여 LayoutLM 모델에 대한 다양한 초기화 방법을 비교합니다.
 3의 결과는 RoBERTaBASE로 초기화 된 LayoutLMBASE 모델이 F1에서 BERTBASE보다 2.1 점 더 우수한 성능을 보여줍니다. LARGE 설정의 경우 RoBERTaLARGE로 초기화 된 LayoutLMLARGE 모델은 BERTLARGE 모델보다 1.3 점 더 향상되었습니다. 앞으로 특히 LARGE 설정의 경우 초기화로 RoBERTa를 사용하여 더 많은 모델을 사전 훈련 할 것입니다.

![layoutLMv result2](/img/41.png)

### 영수증 이해.
SROIE 데이터 세트를 사용하여 영수증 이해 작업을 평가합니다. 결과는 표 4에 나와 있습니다. SROIE에서 주요 정보 추출 작업의 성능 만 테스트하므로 잘못된 OCR 결과의 영향을 제거하려고합니다. 따라서 우리는 Ground Truth OCR을 사용하여 훈련 데이터를 전처리하고 LayoutLM 모델뿐만 아니라 기준 모델 (BERT 및 RoBERTa)을 사용하여 일련의 실험을 실행합니다. 결과는 1,100 만 문서 이미지로 훈련 된 LayoutLMLARGE 모델이 F1 점수 0.9524를 달성 한 것으로 나타났습니다. 이는 경쟁 순위표에서 1 위보다 훨씬 더 나은 수치입니다. 이 결과는 또한 사전 훈련 된 LayoutLM이 도메인 내 데이터 세트 (FUNSD)에서 잘 수행 될뿐만 아니라 SROIE와 같은 도메인 외부 데이터 세트에서 여러 강력한 기준을 능가한다는 것을 확인합니다.

![layoutLMv result2](/img/42.png)

### 문서 이미지 분류.
마지막으로 RVL-CDIP 데이터 세트를 사용하여 문서 이미지 분류 작업을 평가합니다. 문서 이미지의 내용 대부분이 다양한 스타일과 레이아웃의 텍스트이기 때문에 문서 이미지는 다른 자연 이미지와 다릅니다. 전통적으로 사전 훈련이있는 이미지 기반 분류 모델은 표 5에 나와있는 텍스트 기반 모델보다 훨씬 더 나은 성능을 발휘합니다. BERT 또는 RoBERTa가 이미지 기반 접근 방식보다 성능이 저조하여 텍스트 정보가 충분하지 않다는 것을 알 수 있습니다. 이 작업에는 여전히 레이아웃 및 이미지 기능이 필요합니다. 이 작업에 LayoutLM 모델을 사용하여이 문제를 해결합니다. 결과에 따르면 이미지 기능이 없더라도 LayoutLM은 이미지 기반 접근 방식의 단일 모델보다 여전히 우수한 성능을 보입니다. 이미지 임베딩을 통합 한 후 LayoutLM은 문서 이미지 분류를위한 여러 SOTA 기준보다 훨씬 우수한 94.42 %의 정확도를 달성했습니다. 우리 모델은 "이메일"범주에서 가장 좋은 성능을 발휘하는 반면 "양식"범주에서는 가장 낮은 성능을 보입니다. 사전 훈련 된 LayoutLM 및 이미지 모델을 모두 활용하는 방법과 LayoutLM 모델의 사전 훈련 단계에서 이미지 정보를 포함하는 방법을 추가로 조사 할 것입니다.

![layoutLMv result2](/img/43.png)

# LayoutLMv2
# 101. LayoutLMv2 for VrDU(Visually-rich Document Understanding)

[2012.14740.pdf.zip](101%20LayoutLMv2%20for%20VrDU(Visually-rich%20Document%20Und%20782c9cf4ce4e48cb947d3d5e6caac6f5/2012.14740.pdf.zip)

오역, 의역 있습니다.

# Abstract

더 잘 배워진 [cross-modality](https://www.notion.so/cross-modality-20b8ab6d07cc4fa4be694e01adcc73c7)  interaction 에서, 현재 사용되는 masked visual-language modelling 뿐만 아니라 pre-training 에서 새로운 text image alignment 와 matching Task를 사용한다.

한편으로는, 우선, 이것은 spatial-aware [Self-attention Mechanism](https://www.notion.so/Self-attention-Mechanism-3796849904a64ded9b08839550330136) 을 Transformer architecture 로 통합하는 것으로, 모델은 text-blocks 중에 관련있는 위치적 관계 또한 완벽하게 이해할 수 있다.

# 1. Introduction

VrDU 는 스캔되거나 디지털 형태의 비즈니스 문서(자동적으로 추출될 수 있거나 조직화 될 수 있는 구조화된 정보)를 분석하는 것이 목적이다. 많은 비즈니스 어플리케이션을 위해.

기존의 정보추출 작업과는 달리, VrDU작업은 textual 정보에 의지할 뿐만 아니라, 시작적으로 풍부한 문서를 위한 vital인 정보의 visual 과 layout도 의지한다.

다른 타입의 문서들은 문서 내에서 다른 위치에 놓인 종종 스타일이나 포맷을 결정하는 텍스트 필드를 가리킨다.

그러므로 정확게 관심있는 텍스트 필드 인식하는 것은 분명히 시각적으로 풍부한 문서들의 교차 양식 성질의 이점을 가져온다. 텍스쳐, 비쥬얼과 조인트하게 모델되거나 싱글 프레임워크에서 end-to-end로 학습될 수 있는 레이아웃 정보에서.

최근 VrDU의 프로세스가 주로 두 방향에서 드러나고 있다. 첫번때 방향은 대게 텍스처, 비쥬얼/레이아웃/스타일 정보사이가 얕은 융합에서 빌드되었다. 

이 것들은 프리 트레인된 NLP, CV 모델들에 개별적으로 접근하고 supervised leaning(지도학습)을 위해 multiple 양식의 정보을 조합한다.

성능은 좋았지만 변경된 한 문서로 인해 재학습이 필요하다.

추가적으로, 한 문서 타입에서의 도메인 지식이 쉽게 다른 문서타입으로 변환될 수 없다. 때문에 일반적인 문서 레이아웃에서의 지역 불변성은 완전하게 이용할 수 없다.

이 논문에서는...

텍스트 기반 프리트레인 모델과 달리 2D position embedding, image embedding, 기존 text embedding을 사용한다.

**pre-training 단계에서는 두 트레이닝이 사용된다.**

**1) masked visual-language model**

**2) multi-label document classification**

모델은 pre-trained 되었다/많은 수의 IIT-CDIP 데이터셋 unlabeled scanned document images 로.

, 그리고 몇몇 downstream task들에서 기대할 만한 결과를 얻을 수 있다.

기존 연구의 연장은, 우리는 제안한다/새로운 모델 아키텍처 와 pre-training 들을/ LayoutLMv2 모델에서.

- **vanilla LayoutLM 모델과 다른점**은 / 이미지 임배딩 에서 / fine-tuning stage에서 결합되는,
우리는 통합한다/  이미지 정보를/LayoutLMv2의 프리트레인 단계에서 /정보사이에서 cross-modality를 배운 Transformer architecture 의 이점가져오는 것/이미지와 텍스처 정보를

    Different from the vanilla LayoutLM model where image embeddings are combined in the fine-tuning stage, we integrate the image information in the pre-training stage in LayoutLMv2 by taking advantage of the Transformer architecture to learn the cross-modality interaction between visual and textual information.

추가적으로, 1차원 상대 위치 표현에서 영감을 받아, 우리는 제안한다/ LayoutLMv2를 위한 spatial-aware self-attention mechanism 을/ LayoutLMv2-토큰 쌍을 위한 2차원 상대 위치 표현이 포함된.

2-D 포지션 임베딩이랑 다른점은, 상대 위치 임배딩은 명시적으로 제공한다 / broader view를 / 문맥상 공간 모델링을 위한.

프리트레이닝에서 2가지 새로운 트레이닝을 한다.

첫째, 제안된다/ 텍스트이미지 정렬 계획-이미지에서 텍스트라인을 커버하고 텍스트 사이드에서 토큰이 이미지사이드에 커버 되는지 아닌지 분류 예측을 만드는- 이다.

둘째, 텍스트 이미지 매칭 계획이다.여기서 모델을 만들기 위헤 텍스트-이미지 쌍의 일부 이미지는 다른 문서 이미지로 임의로 대체되어 영상 및 OCR 텍스트가 상관 있는지 여부를 학습하게 합니다.

- Downstream Task 란

    구체적으로 풀고 싶은 문제들을 말한다.

    최근 자연어 처리분야에서 언어모델을 프리트레인 방식을 이용해 학습을 진행하고, 그 후에 **실제로 하고자하는 TASK**를 파인 튜닝 방식을 통해 모델을 업데이트하는 등의 방식을 사용하는데 이때 **TASK**를 Downstream Task라고 한다.

- state-of-the-art

    일명 SOTA, 분야별 딥러닝 모델 정보, 데이터셋 등이 제공되는 플랫폼

    [https://paperswithcode.com/sota](https://paperswithcode.com/sota)

실험 결과는 보여준다/ LayoutLMv2 모델의 뛰어난 강력한 baselines 성능을/ vanilla LayoutLM을 포함해서/ 그리고 새로운 SOTA 결과들을 얻었다/ 이런 downstream VrDU Task들에서, 충분히 이익이 되는 많은 수의 실제 문서 이해 tasks.

논문의 기여 요약:

1. single framework에서 end-to-end로 [cross-modality](https://www.notion.so/cross-modality-20b8ab6d07cc4fa4be694e01adcc73c7)  interaction을 학습하는 문서 text, layout, image 정보를  pre-training 단계에서 통합하기 위한 multi-modal Transformer 모델을 제안한다.
2. masked visual-language model외에도, 우리는 다른 양상(modality) 사이를 정렬을 강제하기 위한, 텍스트 이미지 매칭과 택스트 이미지 정렬하는 새로운 프리트래이닝 전략도 더한다. 한편, 공간 인식 self-attention mechanism도 Transformer architecture로 통합된다.
3. LayoutLMv2 는 기존의 VrDU task들의 강력한 성능의 baseline models 뿐 만 아니라, VrDU에 대해 multi-modal pre-training 의 큰 잠재력을 보여주는 새로운 SOTA 결과를 얻었다.

## 2. Approach

![layoutLMv2 architacture](/img/38.png)

제안된 LayoutLMv2 의 전반적인 형태는 보여준다/ [Figure2]() 에서. 우리는 이섹션에서 모델 아키텍처와 프리트레이닝 tasks를 소개할 것이다.

### 2.1 Model Architecture

multi-modal transformer 를 구축.

텍스트, 이미지, 레이아웃 세 입력을 허용한다.

각 입력들은 임베딩 시퀀스로 변환된다.

**텍스트 임베딩 : WordPiece 토크나이즈 사용한다. 단어를 토크나이즈 하고 맨앞에 [CLS] 토큰 추가, word가 끝날때는 [SEP] 토든을 넣고 맨 끝 [SEP] 토큰 뒤에는 [PAD]를 넣는다.**

$$S = \{[CLS],w1,w2,...,[SEP],[PAD],[PAD],...\}, |S| = L$$

## Result

### FUNSD
표 1은 엔티티 수준 정밀도, 재현율 및 F1 점수를 사용하여 평가 된 FUNSD 데이터 세트의 모델 정확도를 보여줍니다. 텍스트 전용 모델의 경우 UniLMv2 모델은 BASE 및 LARGE 설정 측면에서 BERT 모델보다 큰 차이를 보입니다. 텍스트 + 레이아웃 모델의 경우 LayoutLM 제품군은 특히 LayoutLMv2 모델과 같은 텍스트 전용 기준선에 비해 상당한 성능 향상을 가져옵니다. 최상의 성능은 LayoutLMv2LARGE에 의해 달성되며, 현재 SOTA 결과에 비해 3 % F1 포인트 개선이 관찰됩니다. 이것은 LayoutLMv2의 다중 모드 사전 교육이 다른 양식의 상호 작용에서 더 잘 학습하여 양식 이해 작업에 대한 새로운 SOTA로 이어진다는 것을 보여줍니다.

![layoutLMv2 architacture](/img/44.png)

### CORD
표 2는 CORD 데이터 세트의 엔티티 수준 정밀도, 재현율 및 F1 점수를 제공합니다. LayoutLM 제품군은 BERT 및 UniLMv2, 특히 LayoutLMv2 모델을 포함한 텍스트 전용 사전 학습 모델보다 성능이 크게 뛰어납니다. 기준선에 비해 LayoutLMv2 모델은 "SPADE"디코더 방법뿐만 아니라 "SPADE"디코더에 구축 된 "BROS"접근 방식보다 우수하여 텍스트, 레이아웃 및 이미지 정보에 대한 사전 교육의 효과를 확인합니다.

![layoutLMv2 architacture](/img/45.png)

### SROIE
표 3은 SROIE 도전 과제 3의 엔티티 수준 정밀도, 재현율 및 F1 점수를 나열합니다. 텍스트 전용 사전 훈련 된 언어 모델과 비교하여 LayoutLM 제품군 모델은 교차 모달 상호 작용을 통합하여 크게 향상되었습니다. 또한 동일한 모달 정보를 사용하여 LayoutLMv2 모델은 기존의 다중 모달 접근 방식 (Anonymous, 2021; Yu et al., 2020; Zhang et al., 2020)을 능가하여 모델 효과를 보여줍니다. 결국 LayoutLMv2LARGE 단일 모델은 SROIE 순위표에서 상위 1 위 제출을 이길 수도 있습니다.

![layoutLMv2 architacture](/img/46.png)


### Kleister-NDA
표 4는 Kleister-NDA 데이터 세트의 엔티티 수준 F1 점수를 제공합니다. 레이블이 지정된 답변이 표준 형식으로 정규화되므로 추출 된 날짜 정보를 "YYYY-MM-DD"형식으로 변환하는 사후 처리 휴리스틱을 적용하고 회사 이름을 "LLC"및 "Inc"와 같은 약어로 변환합니다. .”. 현재 테스트 세트에 대한 실측 레이블 및 제출 웹 사이트를 사용할 수 없기 때문에 검증 세트에 대한 평가 결과를보고합니다. 실험 결과는 LayoutLMv2 모델이 긴 NDA 문서에 대해 텍스트 전용 및 바닐라 Lay-outLM 모델을 크게 개선하는 것으로 나타났습니다.

![layoutLMv2 architacture](/img/47.png)

### RVL-CDIP
표 5는 텍스트 전용 사전 학습 된 모델, LayoutLM 제품군 및 여러 이미지 기반 기준 모델을 포함한 RVL-CDIP 데이터 세트의 분류 정확도를 보여줍니다. 표에서 볼 수 있듯이 문서 이미지는 텍스트 집약적이며 다양한 레이아웃과 형식으로 표현되기 때문에 텍스트 및 이미지 정보는 문서 이미지 분류 작업에 중요합니다. 따라서 LayoutLM 제품군은 문서 내에서 다중 모달 정보를 활용하기 때문에 이러한 텍스트 전용 또는 이미지 전용 모델보다 성능이 뛰어납니다. 특히 LayoutLMv2LARGE 모델은 95.64 %의 정확도를 달성하는 이전 SOTA 결과에 비해 분류 정확도를 1.2 % F1 포인트 이상 크게 향상시킵니다. 이는 또한 사전 훈련 된 LayoutLMv2 모델이 문서 이해의 정보 추출 작업뿐만 아니라 다양한 양식에 걸친 효과적인 모델 훈련을 통해 문서 이미지 분류 작업에 도움이되는지 확인합니다. 

![layoutLMv2 architacture](/img/48.png)

### DocVQA
표 6에는 DocVQA 데이터 세트의 텍스트 전용 기준선, LayoutLM 제품군 모델 및 리더 보드의 이전 상위 1 위에 대한 평균 정규화 된 Levenshtein 유사성 (ANLS) 점수가 나열되어 있습니다. 다중 모달 사전 교육을 사용하는 LayoutLMv2 모델은 기차 세트에서 미세 조정될 때 LayoutLM 모델 및 텍스트 전용 기준선보다 큰 차이를 보입니다. 모든 데이터 (train + dev)를 미세 조정 데이터 세트로 사용함으로써 LayoutLMv2LARGE 단일 모델은 30 개 모델을 통합하는 리더 보드의 이전 상위 1 개 모델을 능가합니다. QG (질문 생성 데이터 세트) 및 DocVQA 데이터 세트에 대한 LayoutLMv2LARGE 미세 조정 설정에서 단일 모델 성능이 1.6 % ANLS 이상 증가하고 새로운 SOTA를 달성합니다.

![layoutLMv2 architacture](/img/49.png)



#### Reference
> - LayoutLM: Pre-training of Text and Layout for Document Image Understanding
> - LAYOUTLMV2: MULTI-MODAL PRE-TRAINING FOR VISUALLY-RICH DOCUMENT UNDERSTANDING







